{"cells":[{"cell_type":"markdown","metadata":{},"source":"Speech Signal Analysis\n======================\n\n"},{"cell_type":"markdown","metadata":{},"source":["## Introduction\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["%load_ext autoreload\n%autoreload 2"]},{"cell_type":"markdown","metadata":{},"source":["Speech is produced by the excitation of the time-varying vocal tract system by a\ntime-varying source (vibrations of vocal cords). The excitation is generated by\nair flow from the lungs carried by the trachea through the vocal cords. As the\nacoustic wave passes through the vocal tract, its frequency content (spectrum)\nis altered by the resonances of the vocal tract. Vocal tract resonances are\ncalled formants. Thus, the vocal tract shape can be estimated from the spectral\nshape (e.g. formant location and spectral tilt) of the speech signal. The speech\nproduced is an acoustic wave that is recorded, sampled, quantized and stored on\nthe computer as a sequence of numbers (signal). The speech signal can't be used\ndirectly, as the information is in the sequence of the numbers. So the speech\nsignal has to be processed and then features relevant to the task have to be\nextracted. The extracted features may be related to the voice source, i.e. vocal\ncords, like pitch frequency, pitch frequency contour etc. or the vocal tract\nsystem, like linear prediction parameters, cepstral etc. In this laboratory\nsession, we are going to learn about speech signal processing and extraction of\nfeatures related to voice source and vocal-tract system.\n\nWe will conduct the following experiments:\n\n1.  In a 2 second speech signal, you will observe that there is more energy in\n    speech regions than in non-speech regions.\n2.  The speech signal is non-stationary in nature, so it is processed as a\n    short-time signal where it is quasi-stationary. In the second experiment, we\n    will select a short-time speech signal and estimate the pitch frequency\n    manually.\n3.  We will observe the autocorrelation of the short-time signal and compute the\n    pitch frequency from it.\n4.  We will estimate the Fourier spectrum of the short-time signal and also study\n    the effect of windowing.\n5.  We will study the spectrogram of the speech signal observed in the first experiment.\n6.  We will learn about linear prediction analysis and study the vocal-tract response,\n    like formants and voice source feature like pitch frequency.\n7.  We will perform linear prediction analysis on different speech\n    sound signals and observe that they have distinct features.\n8.  Although the features are distinctive in nature they can vary, which makes tasks such as\n    speech recognition or speaker recognition difficult. Thus we will study\n    variability introduced by speakers.\n9.  Finally, we will study the pitch contour and the information embedded in it.\n\nThe sampling frequency *sf* of all speech signals is 16000 Hz. Every file name\ncontains information about the gender, speaker, trial number and the sound, for\nexample the speech signal file `f_s1_t1_a` means it is the vowel /​a/ spoken\nby female (`f`), (for male its `m` and child `c`) speaker 1 (`s1`) and trial\nnumber 1 (`t1`).\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["utterance = \"f_s1_t1_a\""]},{"cell_type":"markdown","metadata":{},"source":["If you have problems in using any of the functions, use `help` to\nknow the usage, for example\n\n    help(speech_signal_observation)\n\nwill give the usage of the function `speech_signal_observation()`.\n\nNote: The speech files are stored in ascii format so kindly don't edit\nor tamper with them. In all the experiments, the usage of the function is\nexplained and then an example usage is given. Follow the example usage\nfor now.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Speech Signal Observation\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Plot the 2 second speech utterance using the\n`speech_signal_observation()` function and observe the envelope of the\nsignal. The usage of the function is:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["from idiap_spe.speech_analysis import *\nhelp(speech_signal_observation)"]},{"cell_type":"markdown","metadata":{},"source":["This plots the speech signal specified with `filename` with an optional title\nand returns the speech signal array, for example:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["data = speech_signal_observation(utterance, title=utterance)"]},{"cell_type":"markdown","metadata":{},"source":["The figure shows the speech utterance plotted in the upper part of the figure\nand the short-time energy plotted below, which is the envelope of the speech\nsignal.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Observation of the Short-Time Speech Signal and Manual Pitch Computation\n\n"]},{"cell_type":"markdown","metadata":{},"source":["The speech signal is non-stationary in nature but it can be assumed to be\nquasi-stationary for one to three pitch periods (short-time signal). In this\nexperiment, we are going to observe a short-time speech signal. Select a 30 ms\nwindow from the 2 second speech signal observed in experiment by\nusing the `select_speech()` function:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["help(select_speech)"]},{"cell_type":"markdown","metadata":{},"source":["For example:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["st_data = select_speech(\n    data, 15000, 15480, \"30 ms window of utterance \" + utterance\n)"]},{"cell_type":"markdown","metadata":{},"source":["Observe the damped sinusoids repeated periodically. Find the period of\neach sinusoid (neglect the sinusoids which are not complete in the\nplot) in the following way:\n\n1.  Note down the sample number of the largest peak of each sinusoid.\n2.  Find the number of samples between each of the consecutive peaks. It gives\n    the period of each sinusoid.\n\nAverage these periods to estimate the pitch period, $p_{t}$. Calculate the\nfundamental frequency or pitch frequency $F_{0}$ using the following equation\n($sf$ *is the sampling frequency*)\n\n\\begin{equation}\n\\label{org8752bbd}\n  F_{0} = \\frac{sf}{p_{t}}\n\\end{equation}\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Autocorrelation Analysis\n\n"]},{"cell_type":"markdown","metadata":{},"source":["In this experiment, we compute the autocorrelation of the short-time\nspeech signal obtained from the Experiment using the\n`autocorrelation()` function:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["help(autocorrelation)"]},{"cell_type":"markdown","metadata":{},"source":["For example:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["corr_data = autocorrelation(\n    st_data, 256,\n    \"256-lag autocorrelation of the 30 ms window of utterance \" + utterance\n)"]},{"cell_type":"markdown","metadata":{},"source":["The length of the autocorrelation array is *lag + lag + 1* which is\nsymmetric to the point *lag + 1* (for the above example it is 257). The\nvalue at this point is the energy of the short-time signal for which the\nautocorrelation was computed. The upper plot shows the actual autocorrelation\n(observe the symmetricity) and the plot below shows the right-half symmetry\n(i.e. from *lag + 1* to *lag + lag + 1*). Find the second peak in this plot and note\ndown the lag number, it is the pitch period $p_{t}$. Use equation to\nfind the fundamental frequency $F_{0}$. Compare it with the $F_{0}$ obtained in\nthe previous experiment.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Fourier Spectrum\n\n"]},{"cell_type":"markdown","metadata":{},"source":["In this experiment, we compute the Fourier spectrum of the short-time\nsignal `st_data` obtained in Experiment using the\n`fourier_spectrum()` function:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["help(fourier_spectrum)"]},{"cell_type":"markdown","metadata":{},"source":["It computes the DFT of order `order` of the short-time signal `data`. The order\nof the DFT is generally chosen such that it is a $2^{n}$ value to take advantage\nof the FFT routine. Depending upon the number of samples, select the order of\nFFT which is near to it, for example the 30 ms window we are using has 480\nsamples, so we select an order of 512:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["fourier_spectrum(\n    st_data, 512,\n    title=\"Fourier spectrum of the 30 ms window of utterance \" + utterance\n)"]},{"cell_type":"markdown","metadata":{},"source":["The upper plot shows the 512-point DFT spectrum (observe the symmetricity) and\nthe plot below shows the left symmetry of the plot (from point 1 to 256).\nObserve the spectral peaks, which are the formants (resonances in the vocal\ntract). The 512-point range covers the entire sampling frequency range, i.e.\n16000 Hz, which has redundant information, whereas the plot below covers half of\nthe sampling frequency, i.e. 8000Hz, which is the region of interest (recall the\nsampling theorem).\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Windowed Speech Analysis\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Window the short-time speech signal `st_data` with the Hanning window:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\nhanning_window = np.hanning(len(st_data))\nplt.plot(hanning_window)\nst_data_hanning = st_data * hanning_window"]},{"cell_type":"markdown","metadata":{},"source":["Compute the Fourier spectrum for the windowed short-time signal\n`st_data_hanning`. Observe the difference in the Fourier spectrum of the signal\n`st_data` using a rectangular window (which was implicit when we created it in\nExperiment ) and the signal `st_data_hanning` using the Hanning\nwindow.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["fourier_spectrum(\n    st_data_hanning, 512,\n    title=\"Fourier spectrum of the 30 ms Hanning window of utterance \" + utterance\n)"]},{"cell_type":"markdown","metadata":{},"source":["## Spectrogram\n\n"]},{"cell_type":"markdown","metadata":{},"source":["In this experiment we are going to compute the narrow-band and\nwide-band spectrogram of the entire utterance i.e. the signal `data`\nobtained in Experiment . Recall that in the\nwide-band spectrogram we get good time\nresolution and in the narrow-band spectrogram we get good frequency resolution. The\nspectrogam is computed using the `spectrogram()` function:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["help(spectrogram)"]},{"cell_type":"markdown","metadata":{},"source":["The type of spectrogram depends upon the order `order`. For a wide-band\nspectrogram we need a small window and choose order 256 or 128, which is a\nshort duration. For a narrow-band spectrogram we choose order 1024 or\n2048, which is a long duration, so we loose the time resolution. The function\nuses the Hanning window internally.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Wide-band Spectrogram\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["spectrogram(data, 256, title=\"Wide-band spectrogram of utterance \" + utterance)"]},{"cell_type":"markdown","metadata":{},"source":["### Narrow-band Spectrogram\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["spectrogram(data, 1024, title=\"Narrow-band spectrogram of utterance \" + utterance)"]},{"cell_type":"markdown","metadata":{},"source":["## Linear Prediction (LP) Analysis\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Linear prediction is the most common technique to estimate the\nshape of the vocal tract. A $p\\text{-th}$ order linear prediction expresses\nevery sample as the linear weighted sum of the past $p$ samples. The\nresulting difference equation expressed in the $z\\text{-domain}$ is\n\n$$\nH(z) = \\frac{1}{1 - \\sum_{j = 1}^{p} a_{j}z^{-j}}\n$$\n\nThe idea behind\nlinear prediction analysis is to estimate the $p$ $a_{k}\\text{-s}$\nthat minimize the mean-square error of the prediction. The linear\nprediction error is also called LP residual. The $a_{k}\\text{-s}$\ndetermine the solution of the equation. The solution of the equation\nin the denominator is called pole. A real pole determines the spectral\nroll-off and a complex pole (which always exists with a conjugate)\ndetermines the location of the formant in the LP spectrum. The LP\nspectrum is the Fourier transform of the $a_{k}\\text{-s}$.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### LP Spectrum\n\n"]},{"cell_type":"markdown","metadata":{},"source":["In this experiment, we will observe the LP spectrum of the short-time\nspeech signal obtained from Experiment using\nthe function `lp_spectrum()`:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["help(lp_spectrum)"]},{"cell_type":"markdown","metadata":{},"source":["`lp_order` is the linear prediction order $p$. The default `window` function is\nthe Hanning window. `order` is the FFT order needed to compute the linear\nprediction spectrum from the $a_{k}\\text{-s}$. For example:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["lp_order = 14\n_ = lp_spectrum(st_data, lp_order, 512)"]},{"cell_type":"markdown","metadata":{},"source":["In the figure, you will observe two plots. The upper plot is\nthe Fourier spectrum and the lower plot is the linear prediction spectrum.\nObserve the more prominent spectral peaks (formants) in the linear prediction spectrum compared\nto the Fourier spectrum. Note down the frequency of each peak, then go back to\nthe wide-band spectrogram from Experiment and observe\nthat the energy is indeed high near that spectral frequency. Now change the linear prediction\norder `lp_order` to, say 1, 3, 16, 20, 30, 50 and observe the changes in\nthe LP spectrum. Try to reason about it.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### LP Residual\n\n"]},{"cell_type":"markdown","metadata":{},"source":["In this experiment we will perform linear\nprediction analysis and compute the LP residual of the short-time speech signal\nobtained from Experiment with the `lp_residual()` function:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["help(lp_residual)"]},{"cell_type":"markdown","metadata":{},"source":["For example:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["lp_order = 10\nresidual = lp_residual(st_data, lp_order)"]},{"cell_type":"markdown","metadata":{},"source":["1.  Note down the sample number of the largest peak of each sinusoid in\n    the upper plot.\n2.  Note down the sample number of the corresponding peaks in the LP residual.\n3.  Compare these two observations. Are they the same?\n\nPerform an autocorrelation analysis on the residual signal using\nthe `autocorrelation()` function and find the pitch period as it was done\nin Experiment :\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["_ = autocorrelation(residual, 256, title=\"Autocorrelation of the LP residual\")"]},{"cell_type":"markdown","metadata":{},"source":["## LP Spectrum of Different Speech Sounds\n\n"]},{"cell_type":"markdown","metadata":{},"source":["In Experiment , we studied the LP spectrum of a short-time signal. In\nthis experiment, we are going to study the LP spectra of different vowels. Note\ndown your observations.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### /​a/\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["st_data_a = load_signal(\"m_s2_t1_a\")[9000:9480]\n_ = lp_spectrum(st_data_a, 16, 512)"]},{"cell_type":"markdown","metadata":{},"source":["### /​e/\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["st_data_e = load_signal(\"m_s2_t1_e\")[8000:8480]\n_ = lp_spectrum(st_data_e, 16, 512)"]},{"cell_type":"markdown","metadata":{},"source":["### /​i/\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["st_data_i = load_signal(\"m_s2_t1_i\")[14000:14480]\n_ = lp_spectrum(st_data_i, 14, 512)"]},{"cell_type":"markdown","metadata":{},"source":["### /​o/\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["st_data_o = load_signal(\"m_s2_t1_o\")[12000:12480]\n_ = lp_spectrum(st_data_o, 18, 512)"]},{"cell_type":"markdown","metadata":{},"source":["### /​u/\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["st_data_u = load_signal(\"m_s2_t1_u\")[17000:17480]\n_ = lp_spectrum(st_data_u, 18, 512)"]},{"cell_type":"markdown","metadata":{},"source":["## Intra- and Inter-Speaker Variability\n\n"]},{"cell_type":"markdown","metadata":{},"source":["In Experiments and , we studied the effect of order on linear\n  prediction and also observed that for different sounds the formants are\n  different. In this experiment, we are going to analyse the variability caused\n  by speakers. There are two kinds of speaker variability that are of interest:\n\n1.  **Intra-speaker variability** is the\n    variability introduced by the same speaker while producing the same sound\n    repeatedly.\n2.  **Inter-speaker variability** is the variability introduced by\n    different speakers producing the same sound.\n\nThis can be useful depending upon the type of application, such as in speech\nrecognition it is good if there is no speaker variability, whereas, for speaker\nrecognition inter-speaker variability is very important. Intra-speaker\nvariability is neither useful for speech recognition nor for speaker recognition\napplications.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Intra-Speaker Variability\n\n"]},{"cell_type":"markdown","metadata":{},"source":["For this experiment we use 3 utterances of the same sound /​a/ spoken by the\nsame speaker 3 different times. We will use the `speaker_variation()` function,\nwhich takes the utterance file names and their corresponding starting points\ndefining the short-time signal. A length of 480 samples for the short-time\nsignal is assumed by default.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["help(speaker_variation)"]},{"cell_type":"markdown","metadata":{},"source":["For example:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["speaker_variation([(\"f_s2_t1_a\", 14000),\n                   (\"f_s2_t2_a\", 10000),\n                   (\"f_s2_t3_a\", 12480)])"]},{"cell_type":"markdown","metadata":{},"source":["This computes the LP spectrum of the short-time signal of all 3 utterances and\nplots them in the same figure. Observe that the first two formant regions for all\n3 utterances are almost the same, while this is not the case for higher formants.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["### Inter-Speaker Variability\n\n"]},{"cell_type":"markdown","metadata":{},"source":["Next, we take 3 utterances of the sound /​a/ spoken by a female, male and a child.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["speaker_variation([(\"f_s1_t1_a\", 15000),\n                   (\"m_s2_t1_a\", 9000),\n                   (\"c_s1_t1_a\", 12480)])"]},{"cell_type":"markdown","metadata":{},"source":["Again observe that the first two formant regions for the male and female speaker\nare almost the same. In case of child speech the second formant shifted much\nmore than the first formant. Like in the previous experiment we observe that the\nhigher formant regions are different for different speakers even though the same\nsound /​a/ is being spoken.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## SIFT Algorithm and Pitch Contour\n\n"]},{"cell_type":"markdown","metadata":{},"source":["In this experiment, we extend the idea of pitch estimation using the LP residual\n(see Experiment ) into a pitch estimation algorithm. The pitch frequency\ncan be estimated through the Simple Inverse Filter Tracking (SIFT) algorithm. It\ncomputes the pitch frequency for a given short-time speech signal in the following\nway:\n\n1.  Low-pass-filter the short-time signal.\n2.  Perform LP analysis and obtain the LP residual.\n3.  Perform autocorrelation on the LP residual.\n4.  Find the location of the second peak, make a decision on voicing. If voiced, compute\n    the pitch frequency, else set the pitch frequency to zero.\n\nThe pitch frequency contour for a spoken sentence can be computed by taking a\nshort-time window of, for example, size 30 ms:\n\n1.  Place this window at the beginning of the speech signal and compute the pitch\n    frequency using the SIFT algorithm.\n2.  Shift the window by 10 ms and compute the pitch frequency using the SIFT algorithm.\n3.  Repeat step 2 until the end of the speech signal is reached.\n\nThe 10 ms shift is called a **frame**. So we obtain a pitch frequency for every 10\nms or every frame. The pitch contour is nothing but the array of pitch\nfrequencies obtained for the sequence of frames. For applications like speech or\nspeaker recognition, for every frame a feature parameter vector (e.g. LP\ncoefficients) is obtained. In other words, the feature extraction stage yields a\nsequence of feature parameter vectors $x_1, x_2 \\cdots x_{N-1}, x_N$, where $N$\nis the number of frames.\n\nIn this experiment, first, we are going to observe the pitch contour\nfor two different types of sentences, interrogative and declarative,\nusing the `sift()` function:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["help(sift)"]},{"cell_type":"markdown","metadata":{},"source":["For this study, we will use a 30 ms frame size (480 samples), a shift of 10 ms\n(160 samples) and a linear prediction order of 10.\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["lp_order = 10\nframe_size = 480\nframe_shift = 160"]},{"cell_type":"markdown","metadata":{},"source":["The sentence spoken is an interrogative sentence, *\"Where are you from?\"*:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["_ = sift(\"m_s1_i_sen1\", lp_order, frame_size, frame_shift)"]},{"cell_type":"markdown","metadata":{},"source":["In this plot you see the speech signal and its pitch contour below. Observe the\nrise and fall of the pitch contour across the sentence. This rise and fall of\npitch contour carries information like speaking style, type of sentence,\nemotional status of the speaker etc. Observe the rise of the pitch contour for\nthe word *where* at the beginning of the sentence (in the context of\ninterrogation). If a line is drawn interpolating the peaks and valleys in the\npitch contour, it will have a positive slope. Observe at the end again a fall\nand then a rise of the pitch contour.\n\nThe next sentence is a declarative sentence, *\"I am from India\"*:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["_ = sift(\"m_s1_d_sen1\", lp_order, frame_size, frame_shift)"]},{"cell_type":"markdown","metadata":{},"source":["Observe the rise and fall of pitch contour across the sentence. If a line is\ndrawn interpolating the peaks and valleys in this pitch contour, it will have a\nnegative slope.\n\nNote that the pitch frequency for a single frame is just an information about\nthe speaker. It doesn't convey any information regarding the sentence being\nspoken or its message or the emotional status of speaker. But when a longer\nduration (say 100-300 ms), i.e. a sequence of frames, is considered then we can\nobserve the rise and fall of the pitch contour and derive such information.\nStill, the pitch contour does not convey any information regarding the message\nbeing spoken.\n\nNow we will perform the pitch contour analysis on the same sentences spoken by a\ndifferent speaker:\n\n"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["_ = sift(\"m_s2_i_sen1\", lp_order, frame_size, frame_shift)"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["_ = sift(\"m_s2_d_sen1\", lp_order, frame_size, frame_shift)"]},{"cell_type":"markdown","metadata":{},"source":["Compare these pitch contours to those of the previous speaker who spoke the\nsame sentences. Are they different?  Humans efficiently use the\nspeaking style information which is embedded in the pitch contour to\nrecognize another person.\n\n"]},{"cell_type":"markdown","metadata":{},"source":["## Acknowledgements\n\n"]},{"cell_type":"markdown","metadata":{},"source":["This lab was originally developed by Sacha Krstulović, Hervé Bourlard, and\nMathew Magimai-Doss for the *Speech Processing and Speech Recognition* course at\nÉcole polytechnique fédérale de Lausanne (EPFL).\n\n"]}],"metadata":{"org":null,"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.5.2"}},"nbformat":4,"nbformat_minor":0}
